{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSpyBik--bLw"
   },
   "source": [
    "# InThon Baseline Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44Og9skt-hdd"
   },
   "source": [
    "## 1. 라이브러리 설치\n",
    "필요한 라이브러리를 설치합니다.\n",
    "\n",
    "먼저, 이 코드는 Hugging Face의 `peft`(Parameter-Efficient Fine-Tuning) 라이브러리를 활용하여 모델 파인튜닝을 합니다. 이를 통해 큰 모델을 메모리 효율적으로 파인튜닝할 수 있습니다. `bitsandbytes`는 모델을 저비트 수 형식(예: 4비트)으로 변환하여 메모리 사용량을 줄입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNnfFxl-diRP"
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/peft.git transformers bitsandbytes datasets\n",
    "!pip install git+https://github.com/salaniz/pycocoevalcap\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, InstructBlipProcessor, InstructBlipForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff8z4SvR-kqR"
   },
   "source": [
    "## 2. 환경 및 디바이스 설정\n",
    "**PyTorch**와 **CUDA**를 사용하여 훈련에 사용할 디바이스를 확인합니다.\n",
    "\n",
    "여기서 `torch.device`를 통해 모델을 `cuda`(GPU)에서 실행할지 `cpu`에서 실행할지를 결정합니다. GPU는 병렬 연산을 빠르게 처리할 수 있기 때문에, 가능하면 GPU를 사용하는 것이 학습 속도에 유리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKwRbavGdkTR"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 디바이스: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sY7S3LFo-o3-"
   },
   "source": [
    "## 3. 모델 및 프로세서 설정\n",
    "`BitsAndBytesConfig`를 사용하여 4비트 양자화를 설정하고, `Blip2Processor`와 `Blip2ForConditionalGeneration`을 설정합니다.\n",
    "\n",
    "일반적으로 모델 파라미터는 32비트로 저장되지만, 이를 4비트로 줄이면 메모리 사용량이 크게 감소합니다. `Blip2Processor`는 이미지와 텍스트를 같이 처리하는 프로세서로, 입력 데이터를 모델이 이해할 수 있는 형태로 변환해 줍니다.\n",
    "\n",
    "### 양자화 (Quantization)\n",
    "**4비트, 8비트 양자화**는 모델의 각 숫자(파라미터)를 4비트로 저장하여 메모리를 절약하는 방법입니다. 모델의 성능에 약간의 손실이 있을 수 있지만, 많은 경우 큰 차이가 없습니다.\n",
    "\n",
    "#### 양자화 원리\n",
    "양자화는 부동소수점 수(floating point)로 표현된 파라미터를 더 적은 비트의 정수(integer)로 근사하여 표현하는 방식입니다. 이때, 수의 범위가 줄어들기 때문에 정밀도가 다소 감소할 수 있지만, 모델의 효율성은 크게 향상됩니다.\n",
    "\n",
    "1. **연속 값에서 이산 값으로 변환**:\n",
    "   주어진 범위 내에서 연속적인 값들을 정해진 개수의 **이산 값(discrete values)**으로 변환합니다. 예를 들어, 4비트 양자화에서는 $2^4 = 16$개의 이산 값으로 모든 파라미터 값을 근사하게 됩니다.\n",
    "\n",
    "2. **스케일링 (Scaling)**:\n",
    "   원래의 실수 값 $x$를 정수 값 $q$로 변환하기 위해, 스케일링 인자 $s$와 오프셋 $z$를 사용하여 양자화를 수행합니다. 변환 수식은 다음과 같습니다.\n",
    "\n",
    "   $$\n",
    "   q = \\text{round}\\left(\\frac{x}{s} + z\\right)\n",
    "   $$\n",
    "\n",
    "   여기서:\n",
    "   - $s$는 스케일링 인자(양자화된 값의 크기를 조절).\n",
    "   - $z$는 오프셋 값으로, 일반적으로 0이나 양자화 범위의 중앙값으로 설정됩니다.\n",
    "   - $\\text{round}$는 반올림 함수로, 실수 값을 가장 가까운 정수 값으로 변환합니다.\n",
    "\n",
    "3. **복원 (Dequantization)**:\n",
    "   양자화된 값을 다시 원래의 값에 가깝게 복원하려면 다음과 같은 복원식을 사용합니다.\n",
    "\n",
    "   $$\n",
    "   x \\approx s \\cdot (q - z)\n",
    "   $$\n",
    "\n",
    "   이를 통해 양자화된 정수 값을 부동소수점 값으로 다시 근사할 수 있으며, 복원된 값은 원래 값과 약간의 차이가 있을 수 있지만, 모델 성능에 큰 영향을 주지 않도록 설계합니다.\n",
    "\n",
    "4. **정밀도와 메모리 효율의 균형**:\n",
    "   양자화는 모델 크기와 메모리 사용을 줄이는 데 효과적이며, 특히 저비트 양자화(예: 4비트)는 모델을 더 작고 빠르게 만듭니다. 4비트 양자화로 인해 수치의 정밀도가 약간 감소하지만, 학습 중 적절한 조정이 이루어지면 성능 저하를 최소화할 수 있습니다.\n",
    "\n",
    "이와 같은 4비트 양자화 기법을 통해 모델의 메모리 효율성을 극대화하면서도, 예측 성능을 유지할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BFTUbVEeCsa"
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    # load_in_4bit=True,\n",
    "    # bnb_4bit_use_double_quant=True,\n",
    "    # bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\")\n",
    "model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/instructblip-flan-t5-xl\",\n",
    "    quantization_config=bnb_config,\n",
    "    # device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXiJ_7KO-2HU"
   },
   "source": [
    "## 4. 모델 준비 및 LoRA 설정\n",
    "`prepare_model_for_kbit_training`로 모델을 준비하고, **LoRA** 파인튜닝을 위한 설정을 합니다.\n",
    "\n",
    "### LoRA (Low-Rank Adaptation)\n",
    "\n",
    "**LoRA**는 큰 모델을 파인튜닝할 때 모든 파라미터를 조정하면 메모리가 많이 필요하므로, 모델의 일부 중요한 파라미터만 선택적으로 학습하여 메모리 효율성을 높이는 기법입니다.\n",
    "\n",
    "#### 이론 및 수식\n",
    "LoRA는 모델의 가중치 행렬 $W \\in \\mathbb{R}^{d \\times k}$을 두 개의 저랭크 행렬 $A \\in \\mathbb{R}^{d \\times r}$과 $B \\in \\mathbb{R}^{r \\times k}$로 근사합니다. 여기서 $r$은 저랭크 차원(rank)을 의미하며, $r$이 작을수록 메모리 절약 효과가 큽니다.\n",
    "\n",
    "1. **가중치 행렬 분해**:\n",
    "   $W$를 LoRA로 근사하기 위해 다음과 같이 표현합니다.\n",
    "   \n",
    "   $$\n",
    "   W \\approx W_0 + \\Delta W = W_0 + A B\n",
    "   $$\n",
    "   \n",
    "   여기서:\n",
    "   - $W_0$는 기존의 고정된 원본 가중치입니다.\n",
    "   - $\\Delta W = A B$는 학습 가능한 저랭크 행렬의 곱으로, 파인튜닝 시 학습됩니다.\n",
    "\n",
    "2. **저랭크 근사**:\n",
    "   저랭크 근사에서 $A$와 $B$의 차원을 $d \\times r$ 및 $r \\times k$로 설정하면, 전체 파라미터 수는 $d \\times k$에서 $d \\times r + r \\times k$로 줄어듭니다. 즉, 전체 파라미터 수는 다음과 같습니다.\n",
    "   \n",
    "   $$\n",
    "   \\text{파라미터 수} = d \\times r + r \\times k\n",
    "   $$\n",
    "\n",
    "   이는 원래 파라미터 수 $d \\times k$에 비해 훨씬 적은 파라미터를 사용하여 근사할 수 있음을 의미합니다.\n",
    "\n",
    "3. **파라미터 효율성**:\n",
    "   LoRA의 저랭크 차원 $r$이 작을수록 메모리 효율성이 높아집니다. 예를 들어 $r$을 $d$나 $k$에 비해 매우 작은 값으로 설정하면, 파라미터 수가 크게 줄어듭니다.\n",
    "\n",
    "이와 같은 LoRA 기법을 통해, 모델의 일부 주요 파라미터를 저랭크 행렬로 근사하여 메모리 사용량을 줄이면서도 파인튜닝 성능을 유지할 수 있습니다.\n",
    "\n",
    "LoRA: https://arxiv.org/abs/2106.09685\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLmB7BH6Id12"
   },
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ToqsDYspeEgt"
   },
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q\", \"k\", \"v\", \"o\"],\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    # task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"LoRA 파인튜닝 설정 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJR8ZYjv-64b"
   },
   "source": [
    "## 5. 학습 가능한 파라미터 확인\n",
    "전체 파라미터 중 학습 가능한 파라미터와 비율을 확인합니다.\n",
    "\n",
    "> 이 부분에 대한 코드는 수정해서는 안 됩니다! 수정을 진행하였을 시 평가에 영향이 있을 수 있습니다. 채점 기준 중 하나인 **학습 가능한 파라미터의 수**와 직결되는 부분입니다. 이와 관련된 부분을 허위로 기재한 것이 적발될 시 평가에 불이익이 있을 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88yX8b81eHqP"
   },
   "outputs": [],
   "source": [
    "def print_trainable_params(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"전체 파라미터 수: {all_params / 1e6:.2f}M\")\n",
    "    print(f\"학습 가능한 파라미터 수: {trainable_params}\")\n",
    "    print(f\"파라미터 비율: {100 * trainable_params / all_params:.2f}%\")\n",
    "\n",
    "print_trainable_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awj7t6jH_W57"
   },
   "source": [
    "## 6. 데이터셋 로드 및 분할\n",
    "데이터셋을 로드하고 학습, 검증, 테스트 세트로 나눕니다.\n",
    "\n",
    "> 이 부분에 대한 코드는 수정해서는 안 됩니다! 수정을 진행하였을 시 평가에 영향이 있을 수 있습니다. 채점 기준 중 하나인 **사용된 데이터셋의 크기**와 직결되는 부분입니다. 이와 관련된 부분을 허위로 기재한 것이 적발될 시 평가에 불이익이 있을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg9Sf-C1jnMa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import random\n",
    "\n",
    "dataset_path = os.path.join('/content', 'dataset.csv')\n",
    "data_df = pd.read_csv(dataset_path)\n",
    "\n",
    "train_df = data_df[data_df['train'] == True]\n",
    "val_df = data_df[data_df['val'] == True]\n",
    "test_df = data_df[data_df['test'] == True]\n",
    "print(f\"Training set size: {len(train_df)}, Validation set size: {len(val_df)}, Test set size: {len(test_df)}\")\n",
    "\n",
    "num_epochs = 2\n",
    "print(f\"Total training data point size: {len(train_df) * num_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voO4tj3D_aDu"
   },
   "source": [
    "## 7. 커스텀 데이터셋 클래스 및 DataLoader 생성\n",
    "\n",
    "### 개념 설명\n",
    "이 단계에서는 `CustomImageCaptionDataset` 클래스를 정의하여 데이터셋을 PyTorch 형식으로 로드합니다. 각 샘플을 이미지와 텍스트 형태로 받아 모델의 입력 형식에 맞게 전처리합니다.\n",
    "\n",
    "### 이론 및 수식\n",
    "텍스트는 정수 인코딩을 통해 모델 입력으로 들어가고, 이미지 데이터는 픽셀 값이 `[0, 1]` 범위로 정규화됩니다.\n",
    "\n",
    "1. **텍스트 인코딩**:\n",
    "   $\n",
    "   \\text{input_ids} = \\text{tokenizer(text)}\n",
    "   $\n",
    "   \n",
    "   `inputs['input_ids']`는 텍스트를 정수 형태로 변환한 결과입니다.\n",
    "\n",
    "2. **이미지 정규화**:\n",
    "   $\n",
    "   \\text{pixel_values} = \\frac{\\text{image} - \\text{min(image)}}{\\text{max(image)} - \\text{min(image)}}\n",
    "   $\n",
    "\n",
    "   이 정규화는 모델이 입력 픽셀값(`inputs['pixel_values']`)을 일정한 범위 내에서 처리할 수 있게 하여 학습의 안정성을 높입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fgg3RNjeeUo-"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "class CustomImageCaptionDataset(Dataset):\n",
    "    def __init__(self, df, processor, test=False):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.test:\n",
    "            image_url = self.df.iloc[idx][\"url\"]\n",
    "            image_id = self.df.iloc[idx][\"Image_ID\"]\n",
    "            caption = self.df.iloc[idx][\"Paragraph\"]\n",
    "\n",
    "            response = requests.get(image_url)\n",
    "            image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "            inputs = self.processor(\n",
    "                images=image,\n",
    "                text=caption,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "            inputs['labels'] = inputs['input_ids'].clone()\n",
    "\n",
    "            inputs['image_url'] = image_url\n",
    "            inputs['Image_ID'] = image_id\n",
    "            inputs['reference_caption'] = caption\n",
    "\n",
    "        else:\n",
    "            image_url = self.df.iloc[idx][\"url\"]\n",
    "            image_id = self.df.iloc[idx][\"Image_ID\"]\n",
    "\n",
    "            inputs = dict()\n",
    "            inputs['image_url'] = image_url\n",
    "            inputs['Image_ID'] = image_id\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aziATQWeglTv"
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomImageCaptionDataset(train_df, processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "\n",
    "val_dataset = CustomImageCaptionDataset(val_df, processor)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=8)\n",
    "\n",
    "test_dataset = CustomImageCaptionDataset(test_df, processor, test=True)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUpx1jKP_hRh"
   },
   "source": [
    "## 8. 옵티마이저와 스케줄러 설정\n",
    "\n",
    "### 개념 설명\n",
    "**옵티마이저**는 모델 파라미터를 조정하는 방법을 정의합니다. **AdamW**는 Adam 옵티마이저의 변형으로, L2 정규화 대신 가중치 감쇠(Weight Decay)를 적용하여 과적합을 방지합니다. **스케줄러**는 학습 중 학습률을 동적으로 조정하여 학습 효율을 높입니다.\n",
    "\n",
    "### 이론 및 수식\n",
    "1. **AdamW 옵티마이저**:\n",
    "   Adam 옵티마이저는 모멘텀을 사용하는 SGD 방식입니다. 각 파라미터에 대해 다음과 같은 방식으로 업데이트가 이루어집니다.\n",
    "\n",
    "   $\n",
    "   m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t\n",
    "   $\n",
    "\n",
    "   $\n",
    "   v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2\n",
    "   $\n",
    "\n",
    "   $\n",
    "   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "   $\n",
    "\n",
    "   $\n",
    "   \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} - \\eta \\lambda \\theta_t\n",
    "   $\n",
    "\n",
    "   여기서 $( \\theta )$는 모델 파라미터, $( g_t )$는 현재 그라디언트, $( \\lambda )$는 가중치 감쇠 계수입니다.\n",
    "\n",
    "2. **스케줄러**: 학습률 감소를 위한 스케줄러는 StepLR을 사용합니다. 이 스케줄러는 주어진 스텝마다 학습률을 감소시킵니다.\n",
    "\n",
    "   $\n",
    "   \\eta_{t+1} = \\gamma \\cdot \\eta_t\n",
    "   $\n",
    "\n",
    "   여기서 $( \\gamma )$는 감소율입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSvAnp4PgqsS"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=total_steps // 2, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1b0IUJh_zhN"
   },
   "source": [
    "## 9. 학습 루프\n",
    "\n",
    "### 1. Autoregressive 모델\n",
    "여기 사용된 모델은 **autoregressive** 모델로, 각 타임 스텝에서 이전 타임 스텝의 출력을 바탕으로 다음 토큰을 순차적으로 예측합니다. 이 방식은 자연어 생성에서 주로 쓰이며, 이미지 캡션 생성 작업에서도 각 단어가 이전 단어와의 관계를 통해 예측됩니다.\n",
    "\n",
    "### 2. 교차 엔트로피 손실 (Cross-Entropy Loss)\n",
    "손실 함수로는 **교차 엔트로피 손실**(Cross-Entropy Loss)을 사용합니다. 모델이 예측한 확률 분포와 실제 정답 레이블 간의 차이를 측정하여, 모델의 예측이 실제 정답과 얼마나 일치하는지 평가합니다.\n",
    "\n",
    "#### 수식\n",
    "주어진 입력 시퀀스 $x$에 대해 모델이 각 시간 단계 $t$에서 다음 토큰 $y_t$를 예측하도록 학습하는 과정에서 교차 엔트로피 손실 $L$은 다음과 같이 정의됩니다.\n",
    "\n",
    "$L = -\\sum_{t=1}^{T} \\log p(y_t | x)$\n",
    "\n",
    "여기서:\n",
    "- $T$는 시퀀스의 길이입니다.\n",
    "- $y_t$는 $t$번째 타임 스텝에서의 실제 정답 토큰입니다.\n",
    "- $p(y_t | x)$는 모델이 $x$를 입력받았을 때 $y_t$ 토큰을 예측할 확률입니다.\n",
    "\n",
    "이 수식에서 모델은 각 토큰의 예측 확률이 실제 정답 토큰에 가까워질수록 손실이 작아지며, 모델이 더 정확해집니다.\n",
    "\n",
    "### 3. Softmax 함수와 Temperature 조정\n",
    "모델은 각 타임 스텝에서 다음 토큰의 확률을 예측할 때 **softmax** 함수를 사용하여 출력 확률 분포를 만듭니다.\n",
    "\n",
    "#### Softmax 함수\n",
    "Softmax는 벡터 $z = (z_1, z_2, \\dots, z_n)$를 입력받아 각 요소의 확률을 계산합니다.\n",
    "\n",
    "$p(y_t | x) = \\frac{\\exp(z_t / T)}{\\sum_{i=1}^{n} \\exp(z_i / T)}$\n",
    "\n",
    "여기서:\n",
    "- $z_i$는 모델의 출력 로짓(logit)입니다.\n",
    "- $T$는 temperature 값으로, softmax의 **temperature**를 조정해 확률 분포의 집중도를 제어할 수 있습니다.\n",
    "\n",
    "#### Temperature의 역할\n",
    "- $T = 1$: 기본적인 softmax로, 확률 분포의 집중도에 변화가 없습니다.\n",
    "- $T > 1$: 확률 분포를 평탄화시켜 더 다양한 토큰이 선택될 가능성을 높여 **탐색적** 예측을 가능하게 합니다.\n",
    "- $T < 1$: 확률 분포를 더 예리하게 만들어 가장 높은 확률을 가진 토큰이 더 자주 선택되며 **보수적** 예측을 가능하게 합니다.\n",
    "\n",
    "이렇게 softmax와 temperature 조정은 모델이 생성하는 문장의 다양성을 조절하는 데 중요한 역할을 합니다.\n",
    "\n",
    "### 4. 코드 내 손실 누적 및 그라디언트 업데이트\n",
    "이 코드에서는 `accumulation_steps`를 통해 여러 미니배치의 손실을 누적하여 메모리 사용량을 조절합니다. 손실은 다음과 같이 누적됩니다.\n",
    "\n",
    "1. **손실 누적**:\n",
    "   $L_{\\text{acc}} = \\sum_{i=1}^{S} L_i$\n",
    "   여기서 $S$는 `accumulation_steps` 값입니다.\n",
    "\n",
    "2. **손실의 평균 계산**:\n",
    "   최종적으로 누적 손실을 $S$로 나눈 평균 손실로 그라디언트를 계산하여 역전파합니다.\n",
    "   $\\bar{L} = \\frac{L_{\\text{acc}}}{S} = \\frac{1}{S} \\sum_{i=1}^{S} L_i$\n",
    "\n",
    "3. **역전파 및 파라미터 업데이트**:\n",
    "   손실 $\\bar{L}$에 대해 그라디언트를 계산하고 역전파를 수행하여 파라미터를 업데이트합니다.\n",
    "\n",
    "이 과정을 통해 모델이 예측 성능을 점차적으로 개선해 나갑니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-V4EYp2w8yL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from types import MethodType\n",
    "\n",
    "def new_forward(self, *args, **kwargs):\n",
    "    if 'inputs_embeds' in kwargs:\n",
    "        kwargs.pop('inputs_embeds')\n",
    "    return self.base_model.forward(*args, **kwargs)\n",
    "\n",
    "model.forward = MethodType(new_forward, model)\n",
    "\n",
    "def forward_pass(input_ids, pixel_values, qformer_input_ids, qformer_attention_mask, labels):\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        pixel_values=pixel_values,\n",
    "        qformer_input_ids=qformer_input_ids,\n",
    "        qformer_attention_mask=qformer_attention_mask,\n",
    "        labels=labels\n",
    "    )\n",
    "    logits = outputs.logits\n",
    "\n",
    "    if logits.size(1) > labels.size(1):\n",
    "        logits = logits[:, :labels.size(1), :]\n",
    "    elif logits.size(1) < labels.size(1):\n",
    "        padding = torch.zeros((logits.size(0), labels.size(1) - logits.size(1), logits.size(2)), device=logits.device)\n",
    "        logits = torch.cat([logits, padding], dim=1)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-X1MlpZgsMy"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "accumulation_steps = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    print(f\"Epoch {epoch+1} / {num_epochs}\")\n",
    "\n",
    "    with tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch+1} (Training)\", postfix={'Train Loss': 0.0}) as pbar:\n",
    "        for idx, batch in enumerate(train_dataloader):\n",
    "            instruction_text = [\"Describe this image in detail.\"] * batch[\"pixel_values\"].size(0)\n",
    "\n",
    "            pixel_values = (batch[\"pixel_values\"] - batch[\"pixel_values\"].min()) / (batch[\"pixel_values\"].max() - batch[\"pixel_values\"].min())\n",
    "            inputs = processor(\n",
    "                images=pixel_values,\n",
    "                text=instruction_text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            loss = forward_pass(\n",
    "                inputs[\"input_ids\"],\n",
    "                inputs[\"pixel_values\"],\n",
    "                inputs.get(\"qformer_input_ids\", None),\n",
    "                inputs.get(\"qformer_attention_mask\", None),\n",
    "                labels\n",
    "            ) / accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if (idx + 1) % accumulation_steps == 0 or (idx + 1 == len(train_dataloader)):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            pbar.set_postfix({'Train Loss': epoch_loss / (idx + 1)})\n",
    "            pbar.update(1)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(val_dataloader), desc=f\"Epoch {epoch+1} (Validation)\", postfix={'Val Loss': 0.0}) as pbar:\n",
    "            for idx, batch in enumerate(val_dataloader):\n",
    "                instruction_text = [\"Describe this image in detail.\"] * batch[\"pixel_values\"].size(0)\n",
    "\n",
    "                pixel_values = (batch[\"pixel_values\"] - batch[\"pixel_values\"].min()) / (batch[\"pixel_values\"].max() - batch[\"pixel_values\"].min())\n",
    "                inputs = processor(\n",
    "                    images=pixel_values,\n",
    "                    text=instruction_text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True\n",
    "                ).to(device)\n",
    "\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                loss = forward_pass(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    inputs[\"pixel_values\"],\n",
    "                    inputs.get(\"qformer_input_ids\", None),\n",
    "                    inputs.get(\"qformer_attention_mask\", None),\n",
    "                    labels\n",
    "                )\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                pbar.set_postfix({'Val Loss': val_loss / (idx + 1)})\n",
    "                pbar.update(1)\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch+1} completed | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tuHBOkh_2iJ"
   },
   "source": [
    "## 10. 평가 및 메트릭 계산\n",
    "테스트 데이터셋에 대해 **SPICE, CLIPScore, CHAIRf** 메트릭을 계산합니다.\n",
    "\n",
    "**SPICE**: 텍스트의 질을 평가하는 점수로, 이미지 캡션의 내용과 관련된 객체, 관계, 속성 등을 기반으로 측정합니다.\n",
    "\n",
    "**CLIPScore**: CLIP 모델을 사용해 이미지와 텍스트의 유사도를 측정하는 점수입니다.\n",
    "\n",
    "**CHAIR**: 캡션이 이미지와 얼마나 잘 일치하는지 평가하는 지표입니다. 특히, 캡션에 어떤 객체가 포함되었는지를 검토하는 데 초점을 둡니다. Precision의 성격이 강한 CHAIRi, CHAIRs와 달리 CHAIRf는 Recall의 성격도 같이 고려하는 평가 지표입니다.\n",
    "\n",
    "SPICE: https://arxiv.org/abs/1607.08822\n",
    "\n",
    "CLIPScore: https://arxiv.org/abs/2104.08718\n",
    "\n",
    "CHAIR: https://arxiv.org/abs/1809.02156\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrFXFyLgguSF"
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "evaluation_objects = [\n",
    "    \"man\", \"woman\", \"tree\", \"sky\", \"building\", \"window\", \"shirt\", \"wall\",\n",
    "    \"sign\", \"grass\", \"water\", \"table\", \"train\", \"plate\", \"car\", \"dog\", \"cat\",\n",
    "    \"giraffe\", \"light\", \"pole\", \"plane\", \"boy\", \"zebra\", \"bus\", \"elephant\",\n",
    "    \"ground\", \"hair\", \"girl\", \"horse\", \"cloud\", \"hand\", \"clock\", \"people\",\n",
    "    \"snow\", \"bird\", \"chair\", \"fence\", \"glass\", \"floor\", \"bear\", \"boat\",\n",
    "    \"street\", \"head\", \"door\", \"road\", \"shoe\", \"leg\", \"eye\", \"hat\"\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "spice_scorer = Spice()\n",
    "\n",
    "def get_singular_form(word):\n",
    "    return word.lemma_\n",
    "\n",
    "def extract_objects_from_caption(caption, object_list):\n",
    "    doc = nlp(caption)\n",
    "    objects_in_caption = set()\n",
    "\n",
    "    for token in doc:\n",
    "        word = token.lemma_.lower()\n",
    "        if word in object_list:\n",
    "            objects_in_caption.add(word)\n",
    "\n",
    "    return objects_in_caption\n",
    "\n",
    "def calculate_chair_metrics(generated_caption, image_objects):\n",
    "    caption_objects = extract_objects_from_caption(generated_caption, evaluation_objects)\n",
    "\n",
    "    hallucinated_objects = caption_objects - set(image_objects)\n",
    "    missing_objects = set(image_objects) - caption_objects\n",
    "    true_positives = caption_objects & set(image_objects)\n",
    "\n",
    "    precision = len(true_positives) / (len(true_positives) + len(hallucinated_objects)) if len(caption_objects) > 0 else 0\n",
    "    recall = len(true_positives) / (len(true_positives) + len(missing_objects)) if len(image_objects) > 0 else 0\n",
    "\n",
    "    chairf = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"환각 객체: {hallucinated_objects}, 누락 객체: {missing_objects}, 캡션 객체: {caption_objects}, 이미지 객체: {image_objects}\")\n",
    "\n",
    "    return chairf\n",
    "\n",
    "def calculate_metrics(image, generated_caption, reference_caption, image_objects):\n",
    "    spice_score, _ = spice_scorer.compute_score({0: [reference_caption]}, {0: [generated_caption]})\n",
    "\n",
    "    inputs = clip_processor(text=generated_caption,\n",
    "                            images=image,\n",
    "                            return_tensors=\"pt\",\n",
    "                            padding=\"max_length\",\n",
    "                            truncation=True,\n",
    "                            max_length=77).to(device)\n",
    "    outputs = clip_model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    clip_score = logits_per_image.item()\n",
    "\n",
    "    chairf = calculate_chair_metrics(generated_caption, image_objects)\n",
    "    print(f'spice_score: {spice_score}, clip_score: {clip_score}, chairf: {chairf}')\n",
    "\n",
    "    return spice_score, clip_score, chairf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTODC3vMuRkn"
   },
   "outputs": [],
   "source": [
    "val_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_dataloader, desc=\"Calculating SPICE, CLIPScore, and CHAIRf\"):\n",
    "        image_urls = batch['image_url']\n",
    "        image_ids = batch['Image_ID']\n",
    "        reference_captions = batch['reference_caption']\n",
    "\n",
    "        images = []\n",
    "        for image_url in image_urls:\n",
    "            response = requests.get(image_url)\n",
    "            image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            images.append(image)\n",
    "\n",
    "        image_objects_batch = [\n",
    "            extract_objects_from_caption(ref_caption, evaluation_objects)\n",
    "            for ref_caption in reference_captions\n",
    "        ]\n",
    "\n",
    "        inputs = processor(images=images, text=['Describe this image in detail.'] * len(images), return_tensors=\"pt\").to(device)\n",
    "\n",
    "        generated_ids = model.generate(**inputs,\n",
    "                                       do_sample=True,\n",
    "                                       num_beams=5,\n",
    "                                       max_length=256,\n",
    "                                       min_length=32,\n",
    "                                       top_p=0.9,\n",
    "                                       repetition_penalty=1.5,\n",
    "                                       length_penalty=1.0,\n",
    "                                       temperature=1)\n",
    "\n",
    "        generated_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        generated_captions = [caption.strip() for caption in generated_captions]\n",
    "\n",
    "        for image_id, generated_caption, reference_caption, image, image_objects in zip(\n",
    "            image_ids, generated_captions, reference_captions, images, image_objects_batch\n",
    "        ):\n",
    "            spice_score, clip_score, chairf = calculate_metrics(\n",
    "                image, generated_caption, reference_caption, image_objects\n",
    "            )\n",
    "\n",
    "            val_results.append({\n",
    "                \"Image_ID\": image_id,\n",
    "                \"generated_caption\": generated_caption,\n",
    "                \"reference_caption\": reference_caption,\n",
    "                \"spice_score\": spice_score,\n",
    "                \"clip_score\": clip_score,\n",
    "                \"chairf\": chairf,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_Xwr5PP6Rfp"
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(val_results)\n",
    "\n",
    "average_spice = results_df[\"spice_score\"].mean()\n",
    "average_clip_score = results_df[\"clip_score\"].mean()\n",
    "average_chairf = results_df[\"chairf\"].mean()\n",
    "\n",
    "print(f\"Average SPICE Score: {average_spice:.4f}\")\n",
    "print(f\"Average CLIPScore: {average_clip_score:.4f}\")\n",
    "print(f\"Average CHAIRf Score: {average_chairf:.4f}\")\n",
    "\n",
    "def calculate_custom_score(spice_score, clip_score, chairf):\n",
    "    custom_score = (0.4 * spice_score) + (0.2 * (clip_score / 250)) + (0.4 * chairf)\n",
    "    return custom_score\n",
    "\n",
    "results_df[\"custom_score\"] = results_df.apply(\n",
    "    lambda row: calculate_custom_score(\n",
    "        row[\"spice_score\"], row[\"clip_score\"], row[\"chairf\"]\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "average_custom_score = results_df[\"custom_score\"].mean()\n",
    "print(f\"Average Custom Score: {average_custom_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78IFiqtTACTx"
   },
   "source": [
    "## 11. 결과 저장 및 평균 점수 출력\n",
    "평균 점수를 출력하고 결과를 CSV 파일로 저장하여 제출에 활용할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-xnmvaSuUqU"
   },
   "outputs": [],
   "source": [
    "submission_data = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Generating Captions\"):\n",
    "        image_urls = batch['image_url']\n",
    "        image_ids = batch['Image_ID']\n",
    "\n",
    "        images = []\n",
    "        for image_url in image_urls:\n",
    "            response = requests.get(image_url)\n",
    "            image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            images.append(image)\n",
    "\n",
    "        inputs = processor(images=images, text=['Describe this image in detail.'] * len(images), return_tensors=\"pt\").to(device)\n",
    "\n",
    "        generated_ids = model.generate(**inputs,\n",
    "                                       do_sample=True,\n",
    "                                       num_beams=5,\n",
    "                                       max_length=256,\n",
    "                                       min_length=32,\n",
    "                                       top_p=0.9,\n",
    "                                       repetition_penalty=1.5,\n",
    "                                       length_penalty=1.0,\n",
    "                                       temperature=1)\n",
    "\n",
    "        generated_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        generated_captions = [caption.strip() for caption in generated_captions]\n",
    "\n",
    "        for image_id, generated_caption in zip(\n",
    "            image_ids, generated_captions\n",
    "        ):\n",
    "            submission_data.append({\n",
    "                \"Image_ID\": image_id,\n",
    "                \"generated_caption\": generated_caption\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruhRNzz8kWDX"
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(submission_data)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YagIvs-L1eAQ"
   },
   "source": [
    "# 예시 1 - 프롬프트 엔지니어링 (Prompt Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OzxBmFpAXPs"
   },
   "source": [
    "## 1. 이미지 불러오기 및 전처리\n",
    "주어진 이미지 URL을 통해 이미지를 불러오고, `processor`를 사용하여 모델의 입력 형식에 맞게 전처리합니다. 이 과정에서 이미지를 RGB로 변환하고, 입력 텐서를 생성하여 모델이 이해할 수 있도록 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4EDtfUaeCYL"
   },
   "outputs": [],
   "source": [
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/LAVIS/assets/merlion.png'\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "raw_image = raw_image.resize((596, 437))\n",
    "display(raw_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgIX-wmOAdfD"
   },
   "source": [
    "## 2. 빔 서치와 누클리어스 샘플링을 통한 캡션 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3P2NxKwAWKl"
   },
   "source": [
    "### 빔 서치 (Beam Search)\n",
    "빔 서치는 여러 후보 캡션 경로를 동시에 탐색하며 최적의 문장을 생성하는 방식입니다. `num_beams=5`로 설정해 5개의 경로를 탐색합니다. 이 방법은 높은 품질의 캡션을 생성하는 데 유리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdZC-1z_eKDz"
   },
   "outputs": [],
   "source": [
    "inputs = processor(images=raw_image, text='Please describe this image briefly.', return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(**inputs,\n",
    "                               num_beams=5,\n",
    "                               max_length=50)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(f\"생성된 캡션 (빔 서치): {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IevfhOpAjJL"
   },
   "source": [
    "### 누클리어스 샘플링 (Nucleus Sampling)\n",
    "누클리어스 샘플링은 특정 누적 확률 (top-p) 이하의 단어들 중에서만 다음 단어를 선택하는 방식입니다. `top_p=0.9`로 설정해, 상위 90% 확률에 해당하는 단어들 중에서만 다음 단어를 샘플링하여 다양한 문장 생성을 유도합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EA-10nVWeNNl"
   },
   "outputs": [],
   "source": [
    "generated_sequences = []\n",
    "for _ in range(3):\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        max_length=50\n",
    "    )\n",
    "    generated_sequences.append(generated_ids)\n",
    "\n",
    "print(\"생성된 캡션 (누클리어스 샘플링):\")\n",
    "for i, generated_ids in enumerate(generated_sequences):\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    print(f\"캡션 {i + 1}: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0UZ5jqoAo8T"
   },
   "source": [
    "## 3. 이미지 기반 Q&A\n",
    "주어진 프롬프트와 함께 이미지를 입력으로 사용하여 질문에 대한 답변을 생성합니다. 프롬프트에 질문-응답 쌍을 기반으로 문맥을 생성하여, 모델이 문맥에 맞는 답변을 생성하도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3T3E1lqqePrV"
   },
   "outputs": [],
   "source": [
    "prompt = \"Question: which city is this? Answer: singapore. Question: why?\"\n",
    "inputs = processor(\n",
    "    images=raw_image,\n",
    "    text=prompt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_length=100)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(f\"생성된 응답: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFR4uEEAAtLw"
   },
   "source": [
    "## 4. 프롬프트 확장 및 이미지 기반 질문 응답\n",
    "문맥을 제공하여 모델이 좀 더 정교한 응답을 생성하도록 합니다. 여기서 주어진 질문-응답 쌍들을 바탕으로 프롬프트를 구성하고, 이어지는 질문에 대해 모델이 답변하도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9xkWqe2eRxj"
   },
   "outputs": [],
   "source": [
    "context = [\n",
    "    (\"which city is this?\", \"singapore\"),\n",
    "    (\"why?\", \"it has a statue of a merlion\"),\n",
    "]\n",
    "question = \"where is the name merlion coming from?\"\n",
    "template = \"Question: {} Answer: {}.\"\n",
    "\n",
    "prompt = \" \".join([template.format(q, a) for q, a in context]) + f\" Question: {question} Answer:\"\n",
    "print(f\"프롬프트: {prompt}\")\n",
    "\n",
    "inputs = processor(\n",
    "    images=raw_image,\n",
    "    text=prompt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_length=100)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(f\"생성된 응답: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yquTWYVhqXhJ"
   },
   "source": [
    "# 예시 2 - 간단한 학습 자유 (training-free) 방법\n",
    "\n",
    "아래 예시는 참가자들의 training-free 방법론에 대한 이해를 돕고자 간단하게 구상한 것입니다. 실제로 효과가 검증이 되어 있지 않는 방법임을 참고하여 주시길 바랍니다. 여러분들의 창의적인 아이디어를 기대하고 있습니다!\n",
    "\n",
    "이 코드는 이미지 캡션 생성을 위해 `generate_with_boost` 함수를 사용해 이미지 및 텍스트 데이터를 처리하고, 다양한 성능 지표를 계산하여 평가합니다. 특히, 임베딩 벡터의 요소별로 평균을 기준으로 가중치를 조정하여 생성된 캡션의 품질에 영향을 줄 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4Qk15bsDzR8"
   },
   "source": [
    "\n",
    "\n",
    "## 1. Boosted Embeddings 설정\n",
    "### `generate_with_boost` 함수\n",
    "이 함수는 모델의 임베딩 가중치를 조정하여 입력 텍스트에 대해 더 집중된 응답을 생성할 수 있도록 합니다. 임베딩 가중치는 전체 임베딩 벡터의 평균을 기준으로 증감되며, **평균보다 큰 값에는 `boost_factor`를 곱하고, 작은 값에는 나눕니다**. 이를 통해 특정 임베딩 벡터의 영향을 강조하거나 약화시킵니다.\n",
    "\n",
    "- `mean_embedding_value`는 임베딩 벡터의 평균값입니다.\n",
    "- `torch.where`를 사용하여 **임베딩 벡터 요소가 평균보다 클 경우 `boost_factor`를 곱하고, 작을 경우 나눕니다**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Z-WFVnsqYUn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_with_boost(model, inputs, boost_factor=1.5):\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_input_embeddings()\n",
    "        mean_embedding_value = embeddings.weight.mean()\n",
    "\n",
    "        boosted_embeddings_weight = torch.where(\n",
    "            embeddings.weight > mean_embedding_value,\n",
    "            embeddings.weight * boost_factor,\n",
    "            embeddings.weight / boost_factor\n",
    "        )\n",
    "\n",
    "        model.get_input_embeddings().weight.data = boosted_embeddings_weight\n",
    "\n",
    "        generated_ids = model.generate(**inputs,\n",
    "                                do_sample=True,\n",
    "                                num_beams=5,\n",
    "                                max_length=256,\n",
    "                                min_length=32,\n",
    "                                top_p=0.9,\n",
    "                                repetition_penalty=1.5,\n",
    "                                length_penalty=1.0,\n",
    "                                temperature=1)\n",
    "        generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    return generated_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7qkKn0hEAvQ"
   },
   "source": [
    "## 2. 데이터 처리 및 캡션 생성\n",
    "이미지 데이터와 참조 캡션을 기반으로 이미지의 객체 정보를 추출하고, 이를 통해 **임베딩 가중치가 조정된 모델로 캡션을 생성**합니다. 결과 캡션과 성능 지표를 저장해 평가할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t721_RcTukAq"
   },
   "outputs": [],
   "source": [
    "val_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, row in tqdm(val_df.iloc[:3].iterrows(), total=len(val_df.iloc[:3]), desc=\"Calculating SPICE, CLIPScore, and CHAIRf\"):\n",
    "        image_url = row['url']\n",
    "        image_id = row['Image_ID']\n",
    "        reference_caption = row['Paragraph']\n",
    "\n",
    "        response = requests.get(image_url)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        image_objects = extract_objects_from_caption(reference_caption, evaluation_objects)\n",
    "\n",
    "        inputs = processor(images=image, text='Describe this image in detail.', return_tensors=\"pt\").to(device)\n",
    "        generated_caption = generate_with_boost(model, inputs, boost_factor=1.5)\n",
    "        print(generated_caption)\n",
    "\n",
    "        spice_score, clip_score, chairf = calculate_metrics(\n",
    "            image, generated_caption, reference_caption, image_objects\n",
    "        )\n",
    "\n",
    "        val_results.append({\n",
    "            \"Image_ID\": image_id,\n",
    "            \"generated_caption\": generated_caption,\n",
    "            \"reference_caption\": reference_caption,\n",
    "            \"spice_score\": spice_score,\n",
    "            \"clip_score\": clip_score,\n",
    "            \"chairf\": chairf\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "cEYBMc2gsWUA"
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(val_results)\n",
    "\n",
    "average_spice = results_df[\"spice_score\"].mean()\n",
    "average_clip_score = results_df[\"clip_score\"].mean()\n",
    "average_chairf = results_df[\"chairf\"].mean()\n",
    "\n",
    "print(f\"Average SPICE Score: {average_spice:.4f}\")\n",
    "print(f\"Average CLIPScore: {average_clip_score:.4f}\")\n",
    "print(f\"Average CHAIRf Score: {average_chairf:.4f}\")\n",
    "\n",
    "def calculate_custom_score(spice_score, clip_score, chairf):\n",
    "    custom_score = (0.4 * spice_score) + (0.2 * (clip_score / 250)) + (0.4 * chairf)\n",
    "    return custom_score\n",
    "\n",
    "results_df[\"custom_score\"] = results_df.apply(\n",
    "    lambda row: calculate_custom_score(\n",
    "        row[\"spice_score\"], row[\"clip_score\"], row[\"chairf\"]\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "average_custom_score = results_df[\"custom_score\"].mean()\n",
    "print(f\"Average Custom Score: {average_custom_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
